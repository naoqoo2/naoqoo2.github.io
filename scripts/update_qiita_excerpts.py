#!/usr/bin/env python3
"""
Qiitaè¨˜äº‹ã‚’ãƒ–ãƒ­ã‚°æŠœç²‹ã«æ›¸ãæ›ãˆã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

- _postsé…ä¸‹ã®Markdownã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ã‚«ãƒ†ã‚´ãƒªãƒ»æœ¬æ–‡ã‚’å–å¾—
- å„è¨˜äº‹ã®è¦ç‚¹ã‚’è‡ªå‹•æŠ½å‡ºã—ã¦ç®‡æ¡æ›¸ãã‚’ç”Ÿæˆ
- Qiita APIçµŒç”±ã§è©²å½“è¨˜äº‹ã®æœ¬æ–‡ã‚’ç½®ãæ›ãˆã‚‹

Usage:
    python3 scripts/update_qiita_excerpts.py           # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆæ›´æ–°ã—ãªã„ï¼‰
    python3 scripts/update_qiita_excerpts.py --apply   # Qiitaè¨˜äº‹ã‚’æ›´æ–°
"""
from __future__ import annotations

import argparse
import json
import re
import ssl
import sys
import time
import unicodedata
import urllib.error
import urllib.parse
import urllib.request
from pathlib import Path
from typing import Dict, List, Optional

BASE_BLOG_URL = "https://naoqoo2.com"
QIITA_API_BASE = "https://qiita.com/api/v2"
TOKEN_PATH = Path(".qiita_token")
POSTS_DIR = Path("_posts")
SITE_DIR = Path("_site")
SSL_CONTEXT = ssl._create_unverified_context()
SITE_TITLE_CACHE: Dict[str, Dict[str, str]] = {}


def read_token() -> str:
    if not TOKEN_PATH.exists():
        raise SystemExit("Qiitaãƒˆãƒ¼ã‚¯ãƒ³(.qiita_token)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    token = TOKEN_PATH.read_text(encoding="utf-8").strip()
    if not token:
        raise SystemExit(".qiita_token ãŒç©ºã§ã™ã€‚")
    return token


def parse_front_matter(text: str) -> Dict[str, object]:
    """yamlãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãªã—ã§å¿…è¦ãªé …ç›®ã ã‘æŠœãå‡ºã™"""
    data: Dict[str, object] = {}
    lines = text.splitlines()
    idx = 0
    while idx < len(lines):
        line = lines[idx]
        if ":" not in line:
            idx += 1
            continue
        key, raw_value = line.split(":", 1)
        key = key.strip()
        value = raw_value.strip()
        if key not in {"title", "categories", "slug"}:
            idx += 1
            continue
        if key in {"title", "slug"}:
            cleaned = value.strip().strip('"').strip("'")
            data[key] = cleaned
            idx += 1
            continue
        # categories
        items: List[str] = []
        if value:
            items.append(value.strip().strip('"').strip("'"))
            idx += 1
            data[key] = items
            continue
        idx += 1
        while idx < len(lines):
            child = lines[idx].strip()
            if not child:
                idx += 1
                continue
            if child.startswith("- "):
                entry = child[2:].strip().strip('"').strip("'")
                if entry:
                    items.append(entry)
                idx += 1
                continue
            break
        data[key] = items
    return data


def slug_from_filename(path: Path) -> str:
    stem = path.stem
    return re.sub(r"^\d{4}-\d{2}-\d{2}-", "", stem)


def category_path(categories: List[str]) -> str:
    if not categories:
        return ""
    normalized = [c.strip("/") for c in categories if c.strip("/")]
    return "/".join(normalized)


def slugify_text(value: str) -> str:
    if not value:
        return ""
    normalized = unicodedata.normalize("NFKC", value).strip()
    normalized = re.sub(r"\s+", "-", normalized)
    normalized = re.sub(r"[^\w\-\u3040-\u30ff\u3400-\u9fff]", "-", normalized, flags=re.UNICODE)
    normalized = re.sub(r"-{2,}", "-", normalized)
    return normalized.strip("-")


def load_site_title_map(cat_path: str) -> Dict[str, str]:
    if cat_path in SITE_TITLE_CACHE:
        return SITE_TITLE_CACHE[cat_path]
    base = SITE_DIR
    if cat_path:
        base = base.joinpath(*cat_path.split("/"))
    mapping: Dict[str, str] = {}
    if not base.exists():
        SITE_TITLE_CACHE[cat_path] = mapping
        return mapping
    for child in base.iterdir():
        if not child.is_dir():
            continue
        index_path = child / "index.html"
        if not index_path.is_file():
            continue
        try:
            html = index_path.read_text(encoding="utf-8", errors="ignore")
        except OSError:
            continue
        title_match = re.search(r"<title>(.*?)</title>", html, flags=re.S)
        if not title_match:
            continue
        title_text = title_match.group(1).strip()
        if " - " in title_text:
            title_text = title_text.split(" - ")[0].strip()
        mapping[title_text] = child.name
    SITE_TITLE_CACHE[cat_path] = mapping
    return mapping


def resolve_slug(title: str, cat_path: str, slug_hint: str) -> str:
    mapping = load_site_title_map(cat_path)
    if title in mapping:
        return mapping[title]
    fallback = slugify_text(slug_hint or title)
    return fallback or slug_hint


def load_posts() -> Dict[str, Dict[str, str]]:
    posts: Dict[str, Dict[str, str]] = {}
    for path in POSTS_DIR.rglob("*.md"):
        text = path.read_text(encoding="utf-8")
        if not text.startswith("---"):
            continue
        parts = text.split("---", 2)
        if len(parts) < 3:
            continue
        front_matter = parse_front_matter(parts[1])
        title = front_matter.get("title")
        if not title:
            continue
        categories = front_matter.get("categories") or []
        if isinstance(categories, str):
            categories = [categories]
        cat_path = category_path(categories)
        slug_hint = front_matter.get("slug") or slug_from_filename(path)
        slug = resolve_slug(title, cat_path, slug_hint)
        posts[title] = {
            "content": parts[2].strip(),
            "category_path": cat_path,
            "slug": slug,
        }
    return posts


def strip_markdown(text: str) -> str:
    text = re.sub(r"```.*?```", "", text, flags=re.S)
    text = re.sub(r"`([^`]*)`", r"\1", text)
    text = re.sub(r"!\[[^\]]*\]\([^)]+\)", "", text)
    text = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", text)
    text = re.sub(r"\*\*([^*]+)\*\*", r"\1", text)
    text = re.sub(r"<br\s*/?>", " ", text, flags=re.I)
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"[*>_#~]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def extract_sections(markdown: str) -> List[Dict[str, List[str]]]:
    sections: List[Dict[str, List[str]]] = []
    lines = markdown.splitlines()
    current = {"heading": None, "content": []}  # type: ignore
    in_code_block = False
    for raw_line in lines:
        line = raw_line.rstrip()
        if line.strip().startswith("```"):
            in_code_block = not in_code_block
            continue
        if in_code_block:
            continue
        heading_match = re.match(r"^(#{2,6})\s+(.*)", line)
        if heading_match:
            if current and (current.get("heading") or any(l.strip() for l in current.get("content", []))):
                sections.append(current)
            current = {"heading": heading_match.group(2).strip(), "content": []}  # type: ignore
            continue
        current.setdefault("content", []).append(line)
    if current and (current.get("heading") or any(l.strip() for l in current.get("content", []))):
        sections.append(current)
    return sections


def summarize_section(section: Dict[str, object]) -> Optional[str]:
    content_lines = []
    for line in section.get("content", []):  # type: ignore
        stripped = line.strip()
        if not stripped:
            continue
        if stripped.startswith(">"):
            stripped = stripped.lstrip(">").strip()
        if stripped.startswith(("-", "*")):
            stripped = stripped.lstrip("-*").strip()
        if stripped.startswith("|") or ("|" in stripped and " | " in stripped):
            continue
        content_lines.append(stripped)
    if not content_lines:
        return None
    text = strip_markdown(" ".join(content_lines))
    if not text:
        return None
    sentences = re.findall(r"[^ã€‚ï¼.!ï¼ï¼Ÿ?]+[ã€‚ï¼.!ï¼ï¼Ÿ?]", text)
    if sentences:
        summary = "".join(sentences[:2]).strip()
    else:
        summary = text.strip()
    if len(summary) > 140:
        summary = summary[:137].rstrip() + "..."
    heading = section.get("heading")
    if heading:
        heading_text = strip_markdown(str(heading))
        if heading_text and not summary.startswith(heading_text):
            summary = f"{heading_text} â€” {summary}"
    return summary


LOW_PRIORITY_HEADINGS = ["ä½™è«‡", "ãŠã‚ã‚Š", "ã¾ã¨ã‚", "æ‰€æ„Ÿ", "ã‚ã¨ãŒã", "é›‘è¨˜"]


def is_low_priority(section: Dict[str, object]) -> bool:
    heading = section.get("heading")
    if not heading:
        return False
    text = str(heading)
    return any(keyword in text for keyword in LOW_PRIORITY_HEADINGS)


def normalize_summary(text: str) -> str:
    cleaned = re.sub(r"\s+", "", text)
    cleaned = re.sub(r"[ã€‚ã€ï¼!ï¼Ÿ?\-ãƒ¼â€•â€”ï¼ˆï¼‰()ãƒ»ã€œï½]", "", cleaned)
    return cleaned


def is_duplicate_summary(seen: List[str], candidate: str) -> bool:
    for existing in seen:
        if candidate == existing:
            return True
        if candidate.startswith(existing) or existing.startswith(candidate):
            return True
    return False


GENERIC_POINT_TEMPLATES = [
    "{title}ã®å®Ÿè£…æ‰‹é †ã‚„ãƒãƒã‚Šã©ã“ã‚ã¯ãƒ–ãƒ­ã‚°æœ¬æ–‡ã§è©³ã—ãç´¹ä»‹ã—ã¦ã„ã¾ã™ã€‚",
    "{title}ã®ã‚³ãƒ¼ãƒ‰å…¨æ–‡ã¨è¨­å®šä¾‹ã¯ãƒ–ãƒ­ã‚°ã‚’ã”è¦§ãã ã•ã„ã€‚",
    "{title}ã®å¿œç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„è£œè¶³ãƒªãƒ³ã‚¯ã‚‚ãƒ–ãƒ­ã‚°ã«è¿½è¨˜ã—ã¦ã„ã¾ã™ã€‚",
]


def extract_points(content: str, title: str, max_points: int = 3) -> List[str]:
    sections = extract_sections(content)
    has_any_heading = any(section.get("heading") for section in sections)
    priority_sections = []
    fallback_sections = []
    for section in sections:
        if section.get("heading") and not is_low_priority(section):
            priority_sections.append(section)
        else:
            fallback_sections.append(section)
    ordered_sections = priority_sections + fallback_sections if priority_sections else fallback_sections
    points: List[str] = []
    normalized_seen: List[str] = []
    for section in ordered_sections:
        summary = summarize_section(section)
        if summary:
            normalized = normalize_summary(summary)
            if is_duplicate_summary(normalized_seen, normalized):
                continue
            normalized_seen.append(normalized)
            points.append(summary)
        if len(points) >= max_points:
            break
    if len(points) < max_points and not has_any_heading:
        fallback = strip_markdown(content)
        sentences = re.findall(r"(.+?[ã€‚ï¼.!ï¼ï¼Ÿ?])", fallback)
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            normalized = normalize_summary(sentence)
            if is_duplicate_summary(normalized_seen, normalized):
                continue
            if len(sentence) < 30:
                continue
            normalized_seen.append(normalized)
            points.append(sentence if len(sentence) <= 140 else sentence[:137] + "...")
            if len(points) >= max_points:
                break
    if not points:
        points.append(f"{title}ã®è©³ç´°ã¯ãƒ–ãƒ­ã‚°ç‰ˆã§è§£èª¬ã—ã¦ã„ã¾ã™ã€‚")
    idx = 0
    while len(points) < max_points and idx < len(GENERIC_POINT_TEMPLATES):
        generic = GENERIC_POINT_TEMPLATES[idx].format(title=title)
        if generic not in points:
            points.append(generic)
        idx += 1
    return points[:max_points]


def build_blog_url(cat_path: str, slug: str) -> str:
    parts: List[str] = []
    if cat_path:
        parts.append(cat_path.strip("/"))
    if slug:
        parts.append(slug.strip("/"))
    encoded_parts = [urllib.parse.quote(p) for p in parts if p]
    path = "/".join(encoded_parts)
    return urllib.parse.urljoin(BASE_BLOG_URL, f"/{path}/")


def build_excerpt(body: str, blog_url: str, title: str) -> str:
    bullets = extract_points(body, title)
    bullet_lines = "\n".join(f"- {point}" for point in bullets)
    more_lines = "\n".join(
        [
            "- ãƒ–ãƒ­ã‚°ã§ã¯ã‚³ãƒ¼ãƒ‰å…¨æ–‡ã‚„è£œè¶³è³‡æ–™ã‚’ç¶™ç¶šçš„ã«æ›´æ–°ã—ã¦ã„ã¾ã™",
            "- æ°—ã«ãªã‚‹ç‚¹ãŒã‚ã‚Œã°ã‚³ãƒ¡ãƒ³ãƒˆãã ã•ã„ğŸ™Œ",
        ]
    )
    return (
        f"> ã“ã®è¨˜äº‹ã¯å€‹äººãƒ–ãƒ­ã‚°ã«ç§»è¡Œã—ã¾ã—ãŸã€‚æœ€æ–°æƒ…å ±ã¯[ãƒ–ãƒ­ã‚°ç‰ˆ]({blog_url})ã‚’ã”è¦§ãã ã•ã„ã€‚\n\n"
        f"## è¦ç‚¹\n{bullet_lines}\n\n"
        f"## ã‚‚ã£ã¨è©³ã—ã\n{more_lines}\n"
    )


def fetch_qiita_items(token: str) -> List[dict]:
    headers = {"Authorization": f"Bearer {token}", "User-Agent": "qiita-excerpt-updater/1.0"}
    items: List[dict] = []
    page = 1
    while True:
        url = f"{QIITA_API_BASE}/authenticated_user/items?page={page}&per_page=100"
        data = request_json(url, headers)
        if not data:
            break
        items.extend(data)
        if len(data) < 100:
            break
        page += 1
    return items


def request_json(url: str, headers: Dict[str, str]) -> List[dict]:
    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, context=SSL_CONTEXT) as resp:
        payload = resp.read()
    return json.loads(payload.decode("utf-8"))


def patch_qiita_item(item: dict, body: str, token: str) -> None:
    item_id = item["id"]
    url = f"{QIITA_API_BASE}/items/{item_id}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
        "User-Agent": "qiita-excerpt-updater/1.0",
    }
    tags = []
    for tag in item.get("tags", []):
        name = tag.get("name")
        if not name:
            continue
        tags.append({"name": name, "versions": tag.get("versions") or []})
    payload_dict = {
        "title": item.get("title"),
        "body": body,
        "tags": tags,
        "private": item.get("private", False),
        "coediting": item.get("coediting", False),
        "gist": item.get("gist", False),
        "tweet": item.get("tweet", False),
    }
    if item.get("group_url_name"):
        payload_dict["group_url_name"] = item["group_url_name"]
    if item.get("organization_url_name"):
        payload_dict["organization_url_name"] = item["organization_url_name"]
    payload = json.dumps(payload_dict).encode("utf-8")
    req = urllib.request.Request(url, data=payload, headers=headers, method="PATCH")
    with urllib.request.urlopen(req, context=SSL_CONTEXT) as resp:
        resp.read()


def main() -> None:
    parser = argparse.ArgumentParser(description="Qiitaè¨˜äº‹ã‚’ãƒ–ãƒ­ã‚°æŠœç²‹ã«æ›´æ–°ã™ã‚‹")
    parser.add_argument("--apply", action="store_true", help="Qiitaè¨˜äº‹ã‚’å®Ÿéš›ã«æ›´æ–°ã™ã‚‹")
    parser.add_argument("--limit", type=int, help="å‡¦ç†ã™ã‚‹è¨˜äº‹æ•°ã‚’åˆ¶é™ã™ã‚‹ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰")
    args = parser.parse_args()

    token = read_token()
    posts = load_posts()
    items = fetch_qiita_items(token)
    if args.limit:
        items = items[: args.limit]

    missing_titles: List[str] = []
    updated_titles: List[str] = []

    for item in items:
        title = item.get("title")
        if title not in posts:
            missing_titles.append(title)
            continue
        post = posts[title]
        blog_url = build_blog_url(post["category_path"], post["slug"])
        body = build_excerpt(post["content"], blog_url, title)
        if args.apply:
            try:
                patch_qiita_item(item, body, token)
                time.sleep(1)
            except urllib.error.HTTPError as err:
                sys.stderr.write(f"[ERROR] {title} ({item['id']}) æ›´æ–°å¤±æ•—: {err.read().decode('utf-8', errors='ignore')}\n")
                continue
        else:
            print("=" * 80)
            print(f"Title: {title}")
            print(body)
        updated_titles.append(title)

    print(f"å¯¾è±¡è¨˜äº‹: {len(items)}ä»¶, æ›´æ–°å¯èƒ½: {len(updated_titles)}ä»¶, æœªãƒãƒƒãƒ: {len(missing_titles)}ä»¶")
    if missing_titles:
        print("æœªãƒãƒƒãƒã‚¿ã‚¤ãƒˆãƒ«:")
        for t in missing_titles:
            print(f"- {t}")


if __name__ == "__main__":
    main()
